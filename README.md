# Natural Language Processing (NLP) Techniques: Tokenization, Lemmatization, and Vectorization:-

## Overview:-
This project focuses on essential Natural Language Processing (NLP) techniques to convert text data into numerical vectors for further analysis. 
Key techniques explored include Tokenization, Lemmatization, and Vectorization. The goal was to understand and implement these concepts to convert 
raw text into machine-readable formats, using methods like Bag of Words, N-grams, TF-IDF, and Word2Vec.

## Techniques Implemented:-

**1. Tokenization**

**2. Lemmatization**

**3. Vectorization**
- Bag of Words (BoW)
- N-grams
- TF-IDF (Term Frequency-Inverse Document Frequency)
- Word2Vec

## Implementation:-

This project was implemented using Python and popular NLP libraries such as:

- `nltk` (Natural Language Toolkit) for tokenization and lemmatization.
- `sklearn` for vectorization techniques like Bag of Words, TF-IDF, and N-grams.
- `gensim` for implementing Word2Vec.
